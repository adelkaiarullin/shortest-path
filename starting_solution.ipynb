{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(1, 30), nn.BatchNorm1d(30), nn.Sigmoid()) \n",
    "                                    #nn.Linear(5, 5), nn.BatchNorm1d(5), nn.ReLU(), nn.Linear(5, 5), nn.BatchNorm1d(5), nn.ReLU(), nn.Linear(5, 5), nn.BatchNorm1d(5), nn.ReLU(),\n",
    "                                    #nn.Linear(5, 5), nn.BatchNorm1d(5), nn.Sigmoid(), nn.Linear(5, 5), nn.BatchNorm1d(5), nn.Sigmoid(), nn.Linear(5, 5), nn.BatchNorm1d(5), nn.Sigmoid())\n",
    "        \n",
    "        self.dist_head = nn.Sequential(nn.Linear(30, 2))\n",
    "        #self.reg_head = nn.Sequential(nn.Linear(5, 5), nn.BatchNorm1d(5), nn.Sigmoid(), nn.Linear(5, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.dist_head(x)#, self.reg_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand((100, 100), device=device)\n",
    "data[data < 0.99] = 0\n",
    "data *= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10:45, 0:15] = torch.rand((35, 15), device=device) * 1000\n",
    "data[30:90, 30:85] = torch.rand((60, 55), device=device) * 1000\n",
    "data[85:95, 85:95] = torch.rand((10, 10), device=device) * 1000\n",
    "# data /= data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semyon/miniconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-93180310de9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbroad_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "start_point = torch.tensor(0.0, device=device)\n",
    "end_point = torch.tensor(1.0, device=device)\n",
    "\n",
    "model = Regressor()\n",
    "reg_loss = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "model = model.to(device)\n",
    "\n",
    "w =  data.shape[1]\n",
    "delta = 1 / data.shape[0]\n",
    "t = torch.linspace(start=0, end=1, steps=data.shape[0], device=device)[..., None]\n",
    "# t = torch.cat([t[..., None], t[..., None]], dim=1)\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for _ in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        output_d = model(t)\n",
    "        dis_norm =  torch.distributions.Normal(output_d[:, 0], output_d[:, 1])\n",
    "        s = dis_norm.rsample()\n",
    "        output_y = s\n",
    "\n",
    "        broad_loss = torch.abs(output_y[0] - start_point) + torch.abs(output_y[-1] - end_point)\n",
    "\n",
    "        broad_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output_d = model(t)\n",
    "    dis_norm =  torch.distributions.Normal(output_d[:, 0], output_d[:, 1])\n",
    "    loss = 0\n",
    "    \n",
    "    for _ in range(10):\n",
    "        s = dis_norm.rsample()\n",
    "        output_y = s\n",
    "        delta_y = output_y[1:] - output_y[:-1]\n",
    "    \n",
    "        index = torch.tensor(output_y.clone().detach().clamp(0, 1 - 1 / w) * w, requires_grad=False, dtype=torch.long, device=device)\n",
    "        z = torch.gather(data, 1, index[..., None])\n",
    "        delta_z = z[1:] - z[:-1]\n",
    "    \n",
    "        length = torch.sqrt(delta ** 2 + delta_y ** 2 + delta_z ** 2).mean()\n",
    "        loss += length\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_y = dis_norm.sample()\n",
    "output_y[0], output_y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output_y.detach().cpu().numpy() * w, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data.cpu().numpy().transpose(1, 0))\n",
    "plt.plot(output_y.detach().cpu().numpy() * w, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0056,  0.0011,  0.0081,  0.0152,  0.0224,  0.0298,  0.0374,  0.0451,\n",
       "          0.0529,  0.0610,  0.0691,  0.0775,  0.0860,  0.0947,  0.1035,  0.1124,\n",
       "          0.1216,  0.1309,  0.1403,  0.1499,  0.1596,  0.1695,  0.1796,  0.1897,\n",
       "          0.2001,  0.2105,  0.2211,  0.2318,  0.2427,  0.2537,  0.2648,  0.2760,\n",
       "          0.2873,  0.2987,  0.3102,  0.3219,  0.3336,  0.3454,  0.3572,  0.3692,\n",
       "          0.3812,  0.3933,  0.4054,  0.4176,  0.4298,  0.4420,  0.4543,  0.4666,\n",
       "          0.4789,  0.4912,  0.5036,  0.5159,  0.5282,  0.5405,  0.5527,  0.5650,\n",
       "          0.5771,  0.5893,  0.6014,  0.6134,  0.6253,  0.6372,  0.6490,  0.6608,\n",
       "          0.6724,  0.6839,  0.6954,  0.7067,  0.7180,  0.7291,  0.7401,  0.7509,\n",
       "          0.7617,  0.7723,  0.7828,  0.7931,  0.8033,  0.8134,  0.8233,  0.8331,\n",
       "          0.8427,  0.8522,  0.8615,  0.8707,  0.8797,  0.8885,  0.8972,  0.9057,\n",
       "          0.9141,  0.9223,  0.9304,  0.9383,  0.9460,  0.9536,  0.9610,  0.9683,\n",
       "          0.9754,  0.9824,  0.9892,  0.9958], grad_fn=<SelectBackward>),\n",
       " tensor([ 1.3345e-03,  1.2353e-03,  1.1351e-03,  1.0337e-03,  9.3110e-04,\n",
       "          8.2751e-04,  7.2268e-04,  6.1681e-04,  5.0978e-04,  4.0156e-04,\n",
       "          2.9226e-04,  1.8185e-04,  7.0199e-05, -4.2513e-05, -1.5651e-04,\n",
       "         -2.7157e-04, -3.8794e-04, -5.0548e-04, -6.2427e-04, -7.4417e-04,\n",
       "         -8.6549e-04, -9.8795e-04, -1.1117e-03, -1.2368e-03, -1.3631e-03,\n",
       "         -1.4908e-03, -1.6198e-03, -1.7501e-03, -1.8816e-03, -2.0145e-03,\n",
       "         -2.1487e-03, -2.2842e-03, -2.4209e-03, -2.5589e-03, -2.6982e-03,\n",
       "         -2.8386e-03, -2.9802e-03, -3.1230e-03, -3.2668e-03, -3.4116e-03,\n",
       "         -3.5573e-03, -3.7039e-03, -3.8514e-03, -3.9996e-03, -4.1485e-03,\n",
       "         -4.2978e-03, -4.4476e-03, -4.5978e-03, -4.7481e-03, -4.8986e-03,\n",
       "         -5.0490e-03, -5.1993e-03, -5.3492e-03, -5.4987e-03, -5.6476e-03,\n",
       "         -5.7957e-03, -5.9430e-03, -6.0891e-03, -6.2341e-03, -6.3776e-03,\n",
       "         -6.5197e-03, -6.6600e-03, -6.7985e-03, -6.9350e-03, -7.0692e-03,\n",
       "         -7.2012e-03, -7.3307e-03, -7.4575e-03, -7.5816e-03, -7.7028e-03,\n",
       "         -7.8210e-03, -7.9360e-03, -8.0477e-03, -8.1560e-03, -8.2609e-03,\n",
       "         -8.3621e-03, -8.4597e-03, -8.5535e-03, -8.6435e-03, -8.7297e-03,\n",
       "         -8.8118e-03, -8.8900e-03, -8.9642e-03, -9.0342e-03, -9.1003e-03,\n",
       "         -9.1622e-03, -9.2201e-03, -9.2739e-03, -9.3236e-03, -9.3692e-03,\n",
       "         -9.4108e-03, -9.4484e-03, -9.4821e-03, -9.5120e-03, -9.5379e-03,\n",
       "         -9.5600e-03, -9.5785e-03, -9.5934e-03, -9.6046e-03, -9.6124e-03],\n",
       "        grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_norm.mean, dis_norm.stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(231.6882, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
